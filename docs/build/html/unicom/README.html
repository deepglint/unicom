

<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Usage &mdash; unicom v1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js?v=99a04888"></script>
      <script src="../_static/doctools.js?v=888ff710"></script>
      <script src="../_static/sphinx_highlight.js?v=4825356b"></script>
      <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html">
            
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <!-- Local TOC -->
              <div class="local-toc"><ul>
<li><a class="reference internal" href="#">Usage</a><ul>
<li><a class="reference internal" href="#api">API</a><ul>
<li><a class="reference internal" href="#unicom-available-models"><code class="docutils literal notranslate"><span class="pre">unicom.available_models()</span></code></a></li>
<li><a class="reference internal" href="#unicom-load-name"><code class="docutils literal notranslate"><span class="pre">unicom.load(name)</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#results-and-evaluation">Results and Evaluation</a><ul>
<li><a class="reference internal" href="#result-transfer-learning-on-imagenet1k">Result Transfer-Learning on ImageNet1K</a></li>
<li><a class="reference internal" href="#result-knn-on-imagenet1k">Result KNN on ImageNet1K</a></li>
<li><a class="reference internal" href="#result-of-supervised-image-retrieval">Result of Supervised Image Retrieval</a></li>
<li><a class="reference internal" href="#result-of-zero-shot-image-retrieval">Result of Zero-Shot Image Retrieval</a></li>
<li><a class="reference internal" href="#eval-image-retrieval">Eval Image Retrieval</a></li>
<li><a class="reference internal" href="#eval-knn">Eval KNN</a></li>
</ul>
</li>
<li><a class="reference internal" href="#vis-zeroshot-retrieval">Vis ZeroShot Retrieval</a><ul>
<li><a class="reference internal" href="#food-101">1. <strong>Food-101</strong></a></li>
<li><a class="reference internal" href="#describable-textures-dataset">2. <strong>Describable Textures Dataset</strong></a></li>
</ul>
</li>
<li><a class="reference internal" href="#googlelandmark">GoogleLandmark</a><ul>
<li><a class="reference internal" href="#googlelandmark-dataset-performance">GoogleLandmark Dataset Performance</a></li>
<li><a class="reference internal" href="#training-instructions">Training Instructions</a></li>
</ul>
</li>
<li><a class="reference internal" href="#citation">Citation</a></li>
</ul>
</div>
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">unicom</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Usage</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/unicom/README.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <p><a class="reference external" href="https://paperswithcode.com/sota/image-retrieval-on-google-landmarks-dataset?p=unicom-universal-and-compact-representation"><img alt="PWC" src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/unicom-universal-and-compact-representation/image-retrieval-on-google-landmarks-dataset" /></a></p>
<p>The model unicom was pre-trained on <a class="reference external" href="https://laion.ai/blog/laion-400-open-dataset/">laion400M</a>, and in the future, we will release the model trained on laion2B.</p>
<section id="usage">
<h1>Usage<a class="headerlink" href="#usage" title="Permalink to this heading"></a></h1>
<p>First, install PyTorch 2.0 (or later) and torchvision, as well as small additional dependencies, and then install this repo as a Python package.
On a CUDA GPU machine, the following will do the trick:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>torch<span class="w"> </span>torchvision
pip<span class="w"> </span>install<span class="w"> </span>tqdm<span class="w"> </span>timm


git<span class="w"> </span>clone<span class="w"> </span>https://github.com/deepglint/unicom
<span class="nb">cd</span><span class="w"> </span>unicom
python
&gt;&gt;&gt;<span class="w"> </span>import<span class="w"> </span>unicom
&gt;&gt;&gt;<span class="w"> </span>unicom.available_models<span class="o">()</span>
<span class="o">[</span><span class="s1">&#39;ViT-B/32&#39;</span>,<span class="w"> </span><span class="s1">&#39;ViT-B/16&#39;</span>,<span class="w"> </span><span class="s1">&#39;ViT-L/14&#39;</span>,<span class="w"> </span><span class="s1">&#39;ViT-L/14@336px&#39;</span><span class="o">]</span>
&gt;&gt;&gt;<span class="w"> </span>unicom.load<span class="o">(</span><span class="s1">&#39;ViT-B/32&#39;</span><span class="o">)</span>
<span class="w">  </span><span class="m">1</span>%<span class="p">|</span>▍<span class="w">                                      </span><span class="p">|</span><span class="w"> </span><span class="m">4</span>.53M/385M<span class="w"> </span><span class="o">[</span><span class="m">00</span>:27&lt;<span class="m">50</span>:34,<span class="w"> </span>132kiB/s<span class="o">]</span>
</pre></div>
</div>
<section id="api">
<h2>API<a class="headerlink" href="#api" title="Permalink to this heading"></a></h2>
<p>The unicom module provides the following methods:</p>
<section id="unicom-available-models">
<h3><code class="docutils literal notranslate"><span class="pre">unicom.available_models()</span></code><a class="headerlink" href="#unicom-available-models" title="Permalink to this heading"></a></h3>
<p>Returns the names of the available unicom models.</p>
</section>
<section id="unicom-load-name">
<h3><code class="docutils literal notranslate"><span class="pre">unicom.load(name)</span></code><a class="headerlink" href="#unicom-load-name" title="Permalink to this heading"></a></h3>
<p>Returns the model and the TorchVision transform needed by the model, specified by the model name returned by <code class="docutils literal notranslate"><span class="pre">unicom.available_models()</span></code>. It will download the model as necessary.</p>
</section>
</section>
</section>
<section id="results-and-evaluation">
<h1>Results and Evaluation<a class="headerlink" href="#results-and-evaluation" title="Permalink to this heading"></a></h1>
<section id="result-transfer-learning-on-imagenet1k">
<h2>Result Transfer-Learning on ImageNet1K<a class="headerlink" href="#result-transfer-learning-on-imagenet1k" title="Permalink to this heading"></a></h2>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Dataset</p></th>
<th class="head"><p>ViT-B/32&#64;384px</p></th>
<th class="head"><p>ViT-B/16&#64;384px</p></th>
<th class="head"><p>ViT-L/14&#64;518px</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>ImageNet1k</p></td>
<td><p>83.6</p></td>
<td><p>85.9</p></td>
<td><p>88.3</p></td>
</tr>
</tbody>
</table>
</section>
<section id="result-knn-on-imagenet1k">
<h2>Result KNN on ImageNet1K<a class="headerlink" href="#result-knn-on-imagenet1k" title="Permalink to this heading"></a></h2>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Dataset</p></th>
<th class="head"><p>ViT-B/32</p></th>
<th class="head"><p>ViT-B/16</p></th>
<th class="head"><p>ViT-L/14</p></th>
<th class="head"><p>ViT-L/14&#64;336px</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>ImageNet1K</p></td>
<td><p>74.5</p></td>
<td><p>78.8</p></td>
<td><p>81.2</p></td>
<td><p>81.6</p></td>
</tr>
</tbody>
</table>
</section>
<section id="result-of-supervised-image-retrieval">
<h2>Result of Supervised Image Retrieval<a class="headerlink" href="#result-of-supervised-image-retrieval" title="Permalink to this heading"></a></h2>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Dataset</p></th>
<th class="head"><p>ViT-B/32</p></th>
<th class="head"><p>ViT-B/16</p></th>
<th class="head"><p>ViT-L/14</p></th>
<th class="head"><p>ViT-L/14&#64;336px</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>SOP</p></td>
<td><p>87.1</p></td>
<td><p>88.8</p></td>
<td><p>89.9</p></td>
<td><p>91.2</p></td>
</tr>
<tr class="row-odd"><td><p>In-Shop</p></td>
<td><p>94.8</p></td>
<td><p>95.5</p></td>
<td><p>96.0</p></td>
<td><p>96.7</p></td>
</tr>
<tr class="row-even"><td><p>INaturalist</p></td>
<td><p>72.8</p></td>
<td><p>82.5</p></td>
<td><p>85.4</p></td>
<td><p>88.9</p></td>
</tr>
</tbody>
</table>
</section>
<section id="result-of-zero-shot-image-retrieval">
<h2>Result of Zero-Shot Image Retrieval<a class="headerlink" href="#result-of-zero-shot-image-retrieval" title="Permalink to this heading"></a></h2>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Dataset</p></th>
<th class="head"><p>ViT-B/32</p></th>
<th class="head"><p>ViT-B/16</p></th>
<th class="head"><p>ViT-L/14</p></th>
<th class="head"><p>ViT-L/14&#64;336px</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>CUB</p></td>
<td><p>83.7</p></td>
<td><p>86.5</p></td>
<td><p>88.5</p></td>
<td><p>89.2</p></td>
</tr>
<tr class="row-odd"><td><p>Cars</p></td>
<td><p>95.9</p></td>
<td><p>96.8</p></td>
<td><p>96.9</p></td>
<td><p>97.3</p></td>
</tr>
<tr class="row-even"><td><p>SOP</p></td>
<td><p>70.0</p></td>
<td><p>70.4</p></td>
<td><p>72.7</p></td>
<td><p>74.5</p></td>
</tr>
<tr class="row-odd"><td><p>In-Shop</p></td>
<td><p>72.8</p></td>
<td><p>74.6</p></td>
<td><p>83.6</p></td>
<td><p>86.7</p></td>
</tr>
<tr class="row-even"><td><p>INaturalist</p></td>
<td><p>64.6</p></td>
<td><p>73.6</p></td>
<td><p>77.1</p></td>
<td><p>81.0</p></td>
</tr>
</tbody>
</table>
</section>
<section id="eval-image-retrieval">
<h2>Eval Image Retrieval<a class="headerlink" href="#eval-image-retrieval" title="Permalink to this heading"></a></h2>
<p>Zero-Shot CUB Dataset with a Single GPU.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>torchrun<span class="w"> </span>retrieval.py<span class="w"> </span>--eval<span class="w"> </span>--dataset<span class="w"> </span>cub<span class="w"> </span>--model_name<span class="w"> </span>ViT-B/32
</pre></div>
</div>
<p>Zero-Shot CUB Dataset with 8 GPUs.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>torchrun<span class="w"> </span>--nproc_per_node<span class="w"> </span><span class="m">8</span><span class="w"> </span>retrieval.py<span class="w"> </span>--eval<span class="w"> </span>--dataset<span class="w"> </span>cub<span class="w"> </span>--model_name<span class="w"> </span>ViT-B/32
</pre></div>
</div>
</section>
<section id="eval-knn">
<h2>Eval KNN<a class="headerlink" href="#eval-knn" title="Permalink to this heading"></a></h2>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>torchrun<span class="w"> </span>--nproc_per_node<span class="w"> </span><span class="m">8</span><span class="w"> </span>knn.py<span class="w"> </span>--train-dataset<span class="w"> </span>/imagenet/train/<span class="w"> </span>--val-dataset<span class="w"> </span>/imagenet/val/<span class="w"> </span>--num-workers<span class="w"> </span><span class="m">4</span><span class="w"> </span>--model-name<span class="w"> </span>ViT-B/32
</pre></div>
</div>
</section>
</section>
<section id="vis-zeroshot-retrieval">
<h1>Vis ZeroShot Retrieval<a class="headerlink" href="#vis-zeroshot-retrieval" title="Permalink to this heading"></a></h1>
<section id="food-101">
<h2>1. <strong>Food-101</strong><a class="headerlink" href="#food-101" title="Permalink to this heading"></a></h2>
<p><img alt="image" src="../_images/vis_food101.jpg" /></p>
</section>
<section id="describable-textures-dataset">
<h2>2. <strong>Describable Textures Dataset</strong><a class="headerlink" href="#describable-textures-dataset" title="Permalink to this heading"></a></h2>
<p><img alt="image" src="../_images/vis_dtd.jpg" /></p>
</section>
</section>
<section id="googlelandmark">
<h1>GoogleLandmark<a class="headerlink" href="#googlelandmark" title="Permalink to this heading"></a></h1>
<section id="googlelandmark-dataset-performance">
<h2>GoogleLandmark Dataset Performance<a class="headerlink" href="#googlelandmark-dataset-performance" title="Permalink to this heading"></a></h2>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Model</p></th>
<th class="head"><p>Public</p></th>
<th class="head"><p>Private</p></th>
<th class="head"><p>Google Drive</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>UNICOM-ViT-B/16&#64;512px</p></td>
<td><p>32.4</p></td>
<td><p>35.7</p></td>
<td><p><a class="reference external" href="https://drive.google.com/file/d/1Vddx3ITUfscXopwcVQGOVESAmcp6M_8t/view?usp=sharing">Click Me</a></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>UNICOM-ViT-L/14&#64;512px</p></td>
<td><p>33.1</p></td>
<td><p>36.4</p></td>
<td><p><a class="reference external" href="https://drive.google.com/file/d/1XCIGmEi6LxGclXuNw3wS_XZlkNSlSQW7/view?usp=sharing">Click Me</a></p></td>
</tr>
</tbody>
</table>
</section>
<section id="training-instructions">
<h2>Training Instructions<a class="headerlink" href="#training-instructions" title="Permalink to this heading"></a></h2>
<p>To successfully train the ViT-L/14 model on the GoogleLandmark dataset, ensure you have access to an NVIDIA A100 GPU with 80GB of memory and PyTorch version 2.0 or higher. Follow these detailed instructions:</p>
<p>Download the Dataset: Obtain the GoogleLandmark dataset and ensure it is stored in a directory accessible to your training environment.<br />
Create the Rec Package: Use the following commands to convert the dataset into a format suitable for training. Replace <code class="docutils literal notranslate"><span class="pre">GLDv2_PATH</span></code> with the actual path to your dataset:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>convert_google_landmark2dali.py<span class="w"> </span>GLDv2_PATH/train_clean.csv<span class="w"> </span>train.lst
python<span class="w"> </span>-m<span class="w"> </span>mxnet.tools.im2rec<span class="w">  </span>--quality<span class="w"> </span><span class="m">100</span><span class="w"> </span>--num-thread<span class="w"> </span><span class="m">32</span><span class="w"> </span>--resize<span class="w"> </span><span class="m">672</span><span class="w"> </span>train.lst<span class="w"> </span>GLDv2_PATH
</pre></div>
</div>
<p>The first command generates a list file (<code class="docutils literal notranslate"><span class="pre">train.lst</span></code>) from the CSV file, which describes the dataset.
The second command converts images to the RecordIO format with specified image quality and size, utilizing multiple threads for efficiency.</p>
<p>After preparing the dataset, you can start training the model with the following command:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>torchrun<span class="w"> </span>--nproc_per_node<span class="w"> </span><span class="m">8</span><span class="w"> </span>finetune_GLDv2.py
</pre></div>
</div>
</section>
</section>
<section id="citation">
<h1>Citation<a class="headerlink" href="#citation" title="Permalink to this heading"></a></h1>
<div class="highlight-latex notranslate"><div class="highlight"><pre><span></span>@inproceedings<span class="nb">{</span>anxiang<span class="nb">_</span>2024<span class="nb">_</span>mlcd,
  title=<span class="nb">{</span>Multi-label Cluster Discrimination for Visual Representation Learning<span class="nb">}</span>,
  author=<span class="nb">{</span>An, Xiang and Yang, Kaicheng and Dai, Xiangzi and Feng, Ziyong and Deng, Jiankang<span class="nb">}</span>,
  booktitle=<span class="nb">{</span>ECCV<span class="nb">}</span>,
  year=<span class="nb">{</span>2024<span class="nb">}</span>
<span class="nb">}</span>
@inproceedings<span class="nb">{</span>anxiang<span class="nb">_</span>2023<span class="nb">_</span>unicom,
  title=<span class="nb">{</span>Unicom: Universal and Compact Representation Learning for Image Retrieval<span class="nb">}</span>,
  author=<span class="nb">{</span>An, Xiang and Deng, Jiankang and Yang, Kaicheng and Li, Jiawei and Feng, Ziyong and Guo, Jia and Yang, Jing and Liu, Tongliang<span class="nb">}</span>,
  booktitle=<span class="nb">{</span>ICLR<span class="nb">}</span>,
  year=<span class="nb">{</span>2023<span class="nb">}</span>
<span class="nb">}</span>
@inproceedings<span class="nb">{</span>anxiang<span class="nb">_</span>2022<span class="nb">_</span>partialfc,
    author=<span class="nb">{</span>An, Xiang and Deng, Jiankang and Guo, Jia and Feng, Ziyong and Zhu, XuHan and Yang, Jing and Liu, Tongliang<span class="nb">}</span>,
    title=<span class="nb">{</span>Killing Two Birds With One Stone: Efficient and Robust Training of Face Recognition CNNs by Partial FC<span class="nb">}</span>,
    booktitle=<span class="nb">{</span>CVPR<span class="nb">}</span>,
    year=<span class="nb">{</span>2022<span class="nb">}</span>,
<span class="nb">}</span>
@inproceedings<span class="nb">{</span>deng<span class="nb">_</span>2019<span class="nb">_</span>arcface,
  title=<span class="nb">{</span>Arcface: Additive angular margin loss for deep face recognition<span class="nb">}</span>,
  author=<span class="nb">{</span>Deng, Jiankang and Guo, Jia and Xue, Niannan and Zafeiriou, Stefanos<span class="nb">}</span>,
  booktitle=<span class="nb">{</span>CVPR<span class="nb">}</span>,
  year=<span class="nb">{</span>2019<span class="nb">}</span>
<span class="nb">}</span>
</pre></div>
</div>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, zhaoyan.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>