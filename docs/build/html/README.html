

<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>UNICOM &amp; MLCD &mdash; unicom v1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />

  
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js?v=99a04888"></script>
      <script src="_static/doctools.js?v=888ff710"></script>
      <script src="_static/sphinx_highlight.js?v=4825356b"></script>
      <script src="_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html">
            
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <!-- Local TOC -->
              <div class="local-toc"><ul>
<li><a class="reference internal" href="#">UNICOM &amp; MLCD</a><ul>
<li><a class="reference internal" href="#latest-news">Latest News</a></li>
<li><a class="reference internal" href="#mlcd-embodied">MLCD-Embodied</a><ul>
<li><a class="reference internal" href="#general-ability-evaluation-comparison-with-llava-onevision-7b-and-gpt-4">General Ability Evaluation: Comparison with LLaVA OneVision-7B and GPT-4</a></li>
<li><a class="reference internal" href="#usage">Usage</a></li>
<li><a class="reference internal" href="#eval">Eval</a></li>
</ul>
</li>
<li><a class="reference internal" href="#multi-label-cluster-discrimination-mlcd">Multi-Label Cluster Discrimination (MLCD)</a><ul>
<li><a class="reference internal" href="#mllms-evaluation-results">MLLMs Evaluation Results</a></li>
<li><a class="reference internal" href="#id1">Usage</a><ul>
<li><a class="reference internal" href="#a-installation">A. Installation</a><ul>
<li><a class="reference internal" href="#clone-this-repository-and-navigate-to-the-llava-folder"><strong>Clone this repository and navigate to the LLaVA folder:</strong></a></li>
</ul>
</li>
<li><a class="reference internal" href="#b-training">B. Training</a></li>
<li><a class="reference internal" href="#c-evaluation">C. Evaluation</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#unicom">UNICOM</a><ul>
<li><a class="reference internal" href="#id2">Usage</a></li>
</ul>
</li>
<li><a class="reference internal" href="#contributors">Contributors</a></li>
<li><a class="reference internal" href="#dataset-contributors">Dataset Contributors</a></li>
<li><a class="reference internal" href="#citation">Citation</a></li>
<li><a class="reference internal" href="#acknowledgement">Acknowledgement</a></li>
</ul>
</li>
</ul>
</div>
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">unicom</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">UNICOM &amp; MLCD</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/README.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <p align="center" width="100%">
<img src="_static/asserts/logo.png" alt="/80dafc65-cda6-4001-aecf-3989ea9d2f7c.webp" width=40%>
</p>
<div>
<section class="tex2jax_ignore mathjax_ignore" id="unicom-mlcd">
<h1>UNICOM &amp; MLCD<a class="headerlink" href="#unicom-mlcd" title="Permalink to this heading">ÔÉÅ</a></h1>
<p><a class="reference external" href="https://arxiv.org/abs/2407.17331"><img alt="Arxiv" src="https://img.shields.io/badge/MLCD-arXiv_2407.17331-red" /></a> <a class="reference external" href="https://arxiv.org/abs/2304.05884"><img alt="Arxiv" src="https://img.shields.io/badge/UNICOM-arXiv_2304.05884-red" /></a> <a class="reference external" href="https://huggingface.co/collections/DeepGlint-AI/mlcd-670d18d767cea37ea7436e69"><img alt="Hugging Face" src="https://img.shields.io/badge/Hugging%20Face-MLCD_Model-yellow" /></a></p>
<p>This repository focuses on building foundational visual models for large multimodal language models using large-scale datasets such as LAION400M and COYO700M. We employ sample-to-cluster contrastive learning to optimize performance. Our models are primarily used for multimodal visual large language models, such as LLaVA.</p>
<p>We adopted the official <a class="reference external" href="https://github.com/LLaVA-VL/LLaVA-NeXT">LLaVA-NeXT</a> and the official training dataset <a class="reference external" href="https://huggingface.co/datasets/lmms-lab/LLaVA-NeXT-Data">LLaVA-NeXT-Data</a> for evaluating the foundational visual models.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Vision Tower</p></th>
<th class="head text-center"><p>RoPE2D</p></th>
<th class="head text-left"><p>ChartQA</p></th>
<th class="head text-left"><p>DocVQA</p></th>
<th class="head text-left"><p>InfoVQA</p></th>
<th class="head text-left"><p>OCRBench</p></th>
<th class="head text-left"><p>MMMU</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>CLIP (ViT-L-14-336px)</p></td>
<td class="text-center"><p>√ó</p></td>
<td class="text-left"><p>66.52</p></td>
<td class="text-left"><p>75.21</p></td>
<td class="text-left"><p>38.88</p></td>
<td class="text-left"><p>525.00</p></td>
<td class="text-left"><p>44.20</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>SigLIP (ViT-SO400M-384px)</p></td>
<td class="text-center"><p>√ó</p></td>
<td class="text-left"><p>69.28</p></td>
<td class="text-left"><p>76.71</p></td>
<td class="text-left"><p>41.38</p></td>
<td class="text-left"><p>554.00</p></td>
<td class="text-left"><p>46.78</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>DFN5B (ViT-H-14-378px)</p></td>
<td class="text-center"><p>√ó</p></td>
<td class="text-left"><p>64.36</p></td>
<td class="text-left"><p>70.87</p></td>
<td class="text-left"><p>38.59</p></td>
<td class="text-left"><p>473.00</p></td>
<td class="text-left"><p><strong>48.00</strong></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong><a class="reference external" href="https://huggingface.co/DeepGlint-AI/mlcd-vit-large-patch14-336">MLCD (ViT-L-14-336px)</a></strong></p></td>
<td class="text-center"><p>√ó</p></td>
<td class="text-left"><p>67.84</p></td>
<td class="text-left"><p>76.46</p></td>
<td class="text-left"><p>43.48</p></td>
<td class="text-left"><p>531.00</p></td>
<td class="text-left"><p>44.30</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong><a class="reference external" href="https://huggingface.co/DeepGlint-AI/mlcd-vit-bigG-patch14-336">MLCD (ViT-bigG-14-336px)</a></strong></p></td>
<td class="text-center"><p>‚àö</p></td>
<td class="text-left"><p><strong>71.07</strong></p></td>
<td class="text-left"><p><strong>79.63</strong></p></td>
<td class="text-left"><p><strong>44.38</strong></p></td>
<td class="text-left"><p><strong>572.00</strong></p></td>
<td class="text-left"><p>46.78</p></td>
</tr>
</tbody>
</table>
<p>The results of the ImageNet linear probe are as follows:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Model Name</p></th>
<th class="head text-center"><p>ImageNet Linear Probe</p></th>
<th class="head text-left"><p>Hugging Face</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>MLCD-ViT-B-32-224px</p></td>
<td class="text-center"><p>79.1</p></td>
<td class="text-left"><p><a class="reference external" href="https://huggingface.co/DeepGlint-AI/mlcd-vit-base-patch32-224">HF:MLCD-ViT-B-32-224px</a></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>MLCD-ViT-L-14-336px</p></td>
<td class="text-center"><p>86.3</p></td>
<td class="text-left"><p><a class="reference external" href="https://huggingface.co/DeepGlint-AI/mlcd-vit-large-patch14-336">HF:MLCD-ViT-L-14-336px</a></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>MLCD-ViT-bigG-14-224px</p></td>
<td class="text-center"><p>87.1</p></td>
<td class="text-left"><p><a class="reference external" href="https://huggingface.co/DeepGlint-AI/mlcd-vit-bigG-patch14-224">HF:MLCD-ViT-bigG-14-224px</a></p></td>
</tr>
</tbody>
</table>
<section id="latest-news">
<h2>Latest News<a class="headerlink" href="#latest-news" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>üéÖ [2024/12] We have launched the <a class="reference external" href="https://github.com/deepglint/unicom/tree/main/downstream">MLCD-Seg-7B</a>, achieving scores of 85.3/81.5 on RefCOCO[testA/B], 82.9/75.6 on RefCOCO+[testA/B], and 80.5 on RefCOCOg[test].  <br>
ü§ñ [2024/11] We have launched the <a class="reference internal" href="#mlcd-embodied"><span class="xref myst">MLCD-Embodied-7B</span></a>, which can reach the level of GPT-4V in embodied capabilities and possesses excellent general understanding abilities. For more details, please click ‚Üí <a class="reference internal" href="MLCD_Embodied.html"><span class="std std-doc">MLCD-Embodied.md</span></a>.  <br>
ü§ó [2024/10] We release <a class="reference external" href="https://huggingface.co/DeepGlint-AI/llava-mlcd-qwen2.5-7b">MLCD-NeXT-7B</a> to Hugging Face.  <br>
üè∞ [2024/07] <a class="reference internal" href="#multi-label-cluster-discrimination-mlcd"><span class="xref myst">MLCD</span></a> was accepted to ECCV2024.  <br>
üåç [2023/03] <a class="reference internal" href="#unicom"><span class="xref myst">UNICOM</span></a> was accepted to ICLR2023.  <br></p>
</section>
<hr class="docutils" />
<section id="mlcd-embodied">
<h2>MLCD-Embodied<a class="headerlink" href="#mlcd-embodied" title="Permalink to this heading">ÔÉÅ</a></h2>
<p><a name="mlcd-embodied"></a>
<a class="reference external" href="https://huggingface.co/DeepGlint-AI/MLCD-Embodied-7B"><img alt="Hugging Face" src="https://img.shields.io/badge/Hugging%20Face-Model-yellow" /></a><br />
More details about MLCD-Embodied can be found in the <a class="reference internal" href="MLCD_Embodied.html"><span class="std std-doc">MLCD-Embodied.md</span></a> file.</p>
<p>Some test results are as follows:</p>
<div style="display: flex; flex-wrap: wrap; justify-content: space-around;">
  <img src="_static/asserts/radar_openeqa.png" alt="Image 1" style="width: 48%;">
  <img src="_static/asserts/radar_robomembar.png" alt="Image 2" style="width: 48%;">
</div>
<section id="general-ability-evaluation-comparison-with-llava-onevision-7b-and-gpt-4">
<h3>General Ability Evaluation: Comparison with LLaVA OneVision-7B and GPT-4<a class="headerlink" href="#general-ability-evaluation-comparison-with-llava-onevision-7b-and-gpt-4" title="Permalink to this heading">ÔÉÅ</a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Dataset</p></th>
<th class="head text-center"><p>Split</p></th>
<th class="head text-center"><p>MLCD-Embodied-7B</p></th>
<th class="head text-center"><p>LLaVA OneVision-7B</p></th>
<th class="head text-center"><p>GPT-4v</p></th>
<th class="head text-center"><p>GPT-4o</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>Vision Encoder</p></td>
<td class="text-center"><p>-</p></td>
<td class="text-center"><p>MLCD-ViT-L-14-336px</p></td>
<td class="text-center"><p>SigLIP</p></td>
<td class="text-center"><p>-</p></td>
<td class="text-center"><p>-</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>ChartQA</p></td>
<td class="text-center"><p>test</p></td>
<td class="text-center"><p>83.0</p></td>
<td class="text-center"><p>80.0</p></td>
<td class="text-center"><p>78.5</p></td>
<td class="text-center"><p>85.7</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>DocVQA</p></td>
<td class="text-center"><p>test</p></td>
<td class="text-center"><p>91.6</p></td>
<td class="text-center"><p>87.5</p></td>
<td class="text-center"><p>88.4</p></td>
<td class="text-center"><p>92.8</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>InfoVQA</p></td>
<td class="text-center"><p>val</p></td>
<td class="text-center"><p>73.9</p></td>
<td class="text-center"><p>70.7</p></td>
<td class="text-center"><p>-</p></td>
<td class="text-center"><p>-</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>InfoVQA</p></td>
<td class="text-center"><p>test</p></td>
<td class="text-center"><p>70.0</p></td>
<td class="text-center"><p>68.8</p></td>
<td class="text-center"><p>-</p></td>
<td class="text-center"><p>-</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>MMMU</p></td>
<td class="text-center"><p>val</p></td>
<td class="text-center"><p>47.3</p></td>
<td class="text-center"><p>48.8</p></td>
<td class="text-center"><p>56.8</p></td>
<td class="text-center"><p>69.1</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>MMStar</p></td>
<td class="text-center"><p>test</p></td>
<td class="text-center"><p>58.5</p></td>
<td class="text-center"><p>61.7</p></td>
<td class="text-center"><p>57.1</p></td>
<td class="text-center"><p>63.9</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>OCRBench</p></td>
<td class="text-center"><p>-</p></td>
<td class="text-center"><p>749.0</p></td>
<td class="text-center"><p>697.0</p></td>
<td class="text-center"><p>656.0</p></td>
<td class="text-center"><p>805.0</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>RealWorldQA</p></td>
<td class="text-center"><p>test</p></td>
<td class="text-center"><p>68.9</p></td>
<td class="text-center"><p>66.3</p></td>
<td class="text-center"><p>61.4</p></td>
<td class="text-center"><p>58.6</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>SeedBench</p></td>
<td class="text-center"><p>image</p></td>
<td class="text-center"><p>74.9</p></td>
<td class="text-center"><p>75.4</p></td>
<td class="text-center"><p>49.9</p></td>
<td class="text-center"><p>76.2</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>MME</p></td>
<td class="text-center"><p>test</p></td>
<td class="text-center"><p>578/1603</p></td>
<td class="text-center"><p>418/1580</p></td>
<td class="text-center"><p>517/1409</p></td>
<td class="text-center"><p>-</p></td>
</tr>
</tbody>
</table>
</section>
<section id="usage">
<h3>Usage<a class="headerlink" href="#usage" title="Permalink to this heading">ÔÉÅ</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/deepglint/unicom
<span class="nb">cd</span><span class="w"> </span>unicom
pip<span class="w"> </span>install<span class="w"> </span>--upgrade<span class="w"> </span>pip
pip<span class="w"> </span>install<span class="w"> </span>-e<span class="w"> </span><span class="s2">&quot;.[train]&quot;</span>
pip<span class="w"> </span>install<span class="w"> </span>flash-attn<span class="w"> </span>--no-build-isolation

<span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span><span class="w"> </span>python<span class="w"> </span>infer.py<span class="w"> </span>--model_dir<span class="w"> </span>DeepGlint-AI/MLCD-Embodied-7B

<span class="c1"># example:</span>
<span class="c1"># &gt;&gt; Enter &#39;exit&#39; to end the conversation, &#39;reset&#39; to clear the chat history.</span>
<span class="c1"># &gt;&gt; Enter image file paths (comma-separated): ./asserts/logo.png</span>
<span class="c1"># &gt;&gt; User: &lt;image&gt;What kind of animal is it in this picture?</span>
<span class="c1"># &gt;&gt; Assistant: The image features a stylized representation of a cat, characterized by its vibrant and abstract depiction.</span>
<span class="c1"># &gt;&gt; User: What color is this cat?</span>
<span class="c1"># &gt;&gt; Assistant: The cat in the image is primarily white with blue, orange and pink accents, creating a visually appealing and unique appearance.</span>
<span class="c1"># &gt;&gt; User: &lt;image&gt;ËØ∑‰Ω†‰ªãÁªç‰∏Ä‰∏ãËøô‰∏™ÂõæÁâá</span>
<span class="c1"># &gt;&gt; Assistant: ËøôÊòØ‰∏ÄÂπÖÂÖÖÊª°ÂàõÊÑèÁöÑÁå´Â§¥Ëâ∫ÊúØ‰ΩúÂìÅ„ÄÇÂÆÉÈááÁî®‰∫ÜÂ§öËâ≤Ê∏êÂèòÂíåÊäΩË±°È£éÊ†ºÔºåÂ∞ÜÁå´ÁöÑÂ§¥ÈÉ®ÊèèÁªòÊàê‰∏Ä‰∏™ÂÖÖÊª°Ê¥ªÂäõÂíåËâ≤ÂΩ©ÁöÑËßÜËßâÂÜ≤Âáª„ÄÇÁå´ÁöÑÁúºÁùõÁî®ÈáëËâ≤Ê∏≤ÊüìÔºåÊòæÂæóÈùûÂ∏∏ÊúâÁ•ûÈááÔºå</span>
<span class="c1"># ËÄåÁ≤âËâ≤ÁöÑÈºªÂ≠êÂàôÂ¢ûÊ∑ª‰∫Ü‰∏Ä‰∏ùÂèØÁà±ÊÑü„ÄÇÊï¥‰ΩìËÆæËÆ°ËûçÂêà‰∫ÜÁé∞‰ª£Ëâ∫ÊúØ‰∏é‰º†ÁªüÁå´Â§¥ÂõæÊ°àÔºåÂàõÈÄ†Âá∫‰∏ÄÁßçÊó¢Áã¨ÁâπÂèàÂºï‰∫∫ÂÖ•ËÉúÁöÑËßÜËßâÊïàÊûú„ÄÇ„ÄÇ</span>
</pre></div>
</div>
</section>
<section id="eval">
<h3>Eval<a class="headerlink" href="#eval" title="Permalink to this heading">ÔÉÅ</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">lmms</span><span class="o">-</span><span class="nb">eval</span><span class="o">==</span><span class="mf">0.2.0</span>
<span class="n">PYTHONPATH</span><span class="o">=./</span> <span class="n">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span> <span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">accelerate</span><span class="o">.</span><span class="n">commands</span><span class="o">.</span><span class="n">launch</span> \
    <span class="o">--</span><span class="n">main_process_port</span><span class="o">=</span><span class="mi">12444</span> \
    <span class="o">--</span><span class="n">num_processes</span><span class="o">=</span><span class="mi">8</span> \
    <span class="o">-</span><span class="n">m</span> <span class="n">lmms_eval</span> \
    <span class="o">--</span><span class="n">model</span> <span class="n">llava</span> \
    <span class="o">--</span><span class="n">model_args</span> <span class="n">pretrained</span><span class="o">=</span><span class="n">DeepGlint</span><span class="o">-</span><span class="n">AI</span><span class="o">/</span><span class="n">MLCD</span><span class="o">-</span><span class="n">Embodied</span><span class="o">-</span><span class="mi">7</span><span class="n">B</span><span class="p">,</span><span class="n">conv_template</span><span class="o">=</span><span class="n">qwen_1_5</span> \
    <span class="o">--</span><span class="n">tasks</span> <span class="n">mme</span> \
    <span class="o">--</span><span class="n">batch_size</span> <span class="mi">1</span> \
    <span class="o">--</span><span class="n">log_samples</span> \
    <span class="o">--</span><span class="n">log_samples_suffix</span> <span class="n">mlcd</span> \
    <span class="o">--</span><span class="n">output_path</span> <span class="o">./</span><span class="n">eval_log</span><span class="o">/</span>
</pre></div>
</div>
</section>
</section>
<section id="multi-label-cluster-discrimination-mlcd">
<h2>Multi-Label Cluster Discrimination (MLCD)<a class="headerlink" href="#multi-label-cluster-discrimination-mlcd" title="Permalink to this heading">ÔÉÅ</a></h2>
<p><a name="multi-label-cluster-discrimination-mlcd"></a>
<a class="reference external" href="https://arxiv.org/abs/2407.17331"><img alt="Arxiv" src="https://img.shields.io/badge/arXiv-2407.17331-red" /></a> <a class="reference external" href="https://huggingface.co/DeepGlint-AI/mlcd-vit-large-patch14-336"><img alt="Hugging Face" src="https://img.shields.io/badge/Hugging%20Face-Model-yellow" /></a><br />
<a class="reference external" href="https://paperswithcode.com/sota/self-supervised-image-classification-on?p=multi-label-cluster-discrimination-for-visual"><img alt="PWC" src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/multi-label-cluster-discrimination-for-visual/self-supervised-image-classification-on" /></a></p>
<p>More details about MLCD can be found in the <a class="reference internal" href="MLCD.html"><span class="std std-doc">MLCD.md</span></a> file.</p>
<p>While CLIP models have shown excellence in many tasks via image-text contrastive learning, they often struggle with encoding complex semantic structures within images. To address this limitation, we introduce <strong>Multi-Label Cluster Discrimination (MLCD)</strong>.</p>
<p>MLCD improves upon traditional approaches by clustering the the LAION dataset, which contains billions of images, into one million centers and assigning multiple closest clusters as labels to each image. This technique accounts for the presence of multiple objects within a single image. We also introduce a novel multi-label classification loss, which separately handles positive and negative class losses, minimizing label ambiguity. Our experiments demonstrate that MLCD achieves state-of-the-art performance in linear probe. Moreover, MLCD shows significant potential when integrated with multimodal large language models. The following two figures compare the evaluation performance of our model on MLLM and Linear Probe. The model we used is ViT-L-14&#64;336px.</p>
<div style="display: flex; flex-wrap: wrap; justify-content: space-around;">
  <img src="_static/asserts/MLCD_Performance_MLLM.png" alt="Image 1" style="width: 49%;">
  <img src="_static/asserts/MLCD_Performance_Linear.png" alt="Image 2" style="width: 49%;">
</div>
<section id="mllms-evaluation-results">
<h3>MLLMs Evaluation Results<a class="headerlink" href="#mllms-evaluation-results" title="Permalink to this heading">ÔÉÅ</a></h3>
<p align="center" width="100%">
<img src="https://github.com/user-attachments/assets/d037ef08-a72f-421a-bdb8-d9b187794989" width="90%">
<p>To evaluate MLCD‚Äôs performance within multimodal large language models (MLLMs), we replaced the CLIP model in LLaVA-NeXT with the MLCD model. We paired this with the Qwen2.5-7B language model. For reproducibility, we utilized the LLaVA-Pretrain dataset for pre-training and the LLaVA-NeXT-Data for structured fine-tuning. The evaluation results confirm that the MLCD model performs exceptionally well across multiple benchmarks, underscoring its effectiveness in MLLMs.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Vision Tower</p></th>
<th class="head text-left"><p>MLCD (ViT_L_14_336px)</p></th>
<th class="head text-left"><p>CLIP (ViT_L_14_336px)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>LLM</p></td>
<td class="text-left"><p>Qwen2.5-7B</p></td>
<td class="text-left"><p>Qwen2.5-7B</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>AI2D</p></td>
<td class="text-left"><p><strong>76.98</strong></p></td>
<td class="text-left"><p>73.15</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>GQA</p></td>
<td class="text-left"><p><strong>64.17</strong></p></td>
<td class="text-left"><p>63.31</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>ScienceQA-Img</p></td>
<td class="text-left"><p><strong>78.09</strong></p></td>
<td class="text-left"><p>76.35</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>InfoVQA-Val</p></td>
<td class="text-left"><p><strong>43.48</strong></p></td>
<td class="text-left"><p>38.88</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>MMBenchCN-Dev</p></td>
<td class="text-left"><p><strong>74.83</strong></p></td>
<td class="text-left"><p>72.51</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>MMBenchEN-Dev</p></td>
<td class="text-left"><p><strong>76.37</strong></p></td>
<td class="text-left"><p>74.57</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>SeedBench</p></td>
<td class="text-left"><p><strong>68.20</strong></p></td>
<td class="text-left"><p>66.80</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>SeedBench-Img</p></td>
<td class="text-left"><p><strong>73.75</strong></p></td>
<td class="text-left"><p>72.72</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>MMStar</p></td>
<td class="text-left"><p><strong>50.98</strong></p></td>
<td class="text-left"><p>48.98</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>MMMU</p></td>
<td class="text-left"><p><strong>44.30</strong></p></td>
<td class="text-left"><p>44.20</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>POPE</p></td>
<td class="text-left"><p>88.69</p></td>
<td class="text-left"><p><strong>88.83</strong></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>ChartQA</p></td>
<td class="text-left"><p><strong>67.84</strong></p></td>
<td class="text-left"><p>66.52</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>DocVQA-Val</p></td>
<td class="text-left"><p><strong>76.46</strong></p></td>
<td class="text-left"><p>75.21</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>TextVQA-Val</p></td>
<td class="text-left"><p>61.69</p></td>
<td class="text-left"><p><strong>62.47</strong></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>OCRBench</p></td>
<td class="text-left"><p><strong>531</strong></p></td>
<td class="text-left"><p>525</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>MME (cognition)</p></td>
<td class="text-left"><p><strong>432</strong></p></td>
<td class="text-left"><p>384</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>MME (perception)</p></td>
<td class="text-left"><p><strong>1598</strong></p></td>
<td class="text-left"><p>1512</p></td>
</tr>
</tbody>
</table>
</section>
<section id="id1">
<h3>Usage<a class="headerlink" href="#id1" title="Permalink to this heading">ÔÉÅ</a></h3>
<section id="a-installation">
<h4>A. Installation<a class="headerlink" href="#a-installation" title="Permalink to this heading">ÔÉÅ</a></h4>
<section id="clone-this-repository-and-navigate-to-the-llava-folder">
<h5><strong>Clone this repository and navigate to the LLaVA folder:</strong><a class="headerlink" href="#clone-this-repository-and-navigate-to-the-llava-folder" title="Permalink to this heading">ÔÉÅ</a></h5>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/deepglint/unicom
<span class="nb">cd</span><span class="w"> </span>unicom

<span class="c1"># Upgrade pip and install necessary dependencies</span>
pip<span class="w"> </span>config<span class="w"> </span><span class="nb">set</span><span class="w"> </span>global.index-url<span class="w"> </span>https://pypi.org/simple
pip<span class="w"> </span>install<span class="w"> </span>--upgrade<span class="w"> </span>pip
pip<span class="w"> </span>install<span class="w"> </span>-e<span class="w"> </span><span class="s2">&quot;.[train]&quot;</span>

<span class="c1"># flash attention</span>
pip<span class="w"> </span>install<span class="w"> </span>flash-attn<span class="w"> </span>--no-build-isolation
</pre></div>
</div>
</section>
</section>
<section id="b-training">
<h4>B. Training<a class="headerlink" href="#b-training" title="Permalink to this heading">ÔÉÅ</a></h4>
<p><strong>Stage 1: MLCD-LLaVA-NeXT Pretraining</strong></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>scripts/pretrain_mlcd.sh
</pre></div>
</div>
<p><strong>Stage 2: MLCD-LLaVA-NeXT Instructional Finetuning</strong></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>scripts/finetune_mlcd.sh
</pre></div>
</div>
</section>
<section id="c-evaluation">
<h4>C. Evaluation<a class="headerlink" href="#c-evaluation" title="Permalink to this heading">ÔÉÅ</a></h4>
<p>Install the evaluation tool and execute the evaluation script:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>lmms-eval<span class="o">==</span><span class="m">0</span>.2.0
bash<span class="w"> </span>eval.sh
</pre></div>
</div>
</section>
</section>
</section>
<hr class="docutils" />
<section id="unicom">
<h2>UNICOM<a class="headerlink" href="#unicom" title="Permalink to this heading">ÔÉÅ</a></h2>
<p><a name="unicom"></a>
<a class="reference external" href="https://arxiv.org/abs/2304.05884"><img alt="Arxiv" src="https://img.shields.io/badge/arXiv-2304.05884-red" /></a> <a class="reference external" href="https://drive.google.com/drive/folders/18wsNgZeNpjKAcIrWoffJ8o9UqmMHUBqN?usp=share_link"><img alt="Google Drive" src="https://img.shields.io/badge/Google%20Drive-Model-yellow" /></a></p>
<p>For image representation:</p>
<ol class="arabic simple">
<li><p>ImageNet pretraining is not universal enough to generalize to diverse open-world objects.</p></li>
<li><p>Supervised learning is not scalable because manual annotation of large-scale training data is time-consuming, costly, and even infeasible.</p></li>
<li><p>Instance discrimination method (e.g., CLIP) can hardly encode the semantic structure of training data, because instance-wise contrastive learning always treats two samples as a negative pair, regardless of their semantic similarity.</p></li>
</ol>
<p>UNICOM demonstrates superior performance in image retrieval, thanks to its ability to cluster <strong>400000000</strong> images into <strong>1000000</strong> pseudo classes using joint textual and visual features extracted by the CLIP model. Additionally, our use of a margin-based softmax loss (ArcFace) and random partial class/feature (PartialFC) selections enhances the robustness and compactness of the feature embedding. Our method outperforms state-of-the-art unsupervised and supervised image retrieval approaches, making it a powerful tool for researchers and practitioners in the field.</p>
<section id="id2">
<h3>Usage<a class="headerlink" href="#id2" title="Permalink to this heading">ÔÉÅ</a></h3>
<p>For detailed instructions, please refer to the UNICOM  <a class="reference internal" href="unicom/README.html"><span class="std std-doc">Documentation</span></a>.</p>
</section>
</section>
<section id="contributors">
<h2>Contributors<a class="headerlink" href="#contributors" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>Thanks so much to all of our amazing contributors!</p>
<!-- readme: contributors -start -->
<table>
	<tbody>
		<tr>
            <td align="center">
                <a href="https://github.com/Barry-Zhou">
                    <img src="https://avatars.githubusercontent.com/u/24220199?v=4" width="100;" alt="Barry-Zhou"/>
                    <br />
                    <sub><b>Barry-Zhou</b></sub>
                </a>
            </td>
            <td align="center">
                <a href="https://github.com/daixiangzi">
                    <img src="https://avatars.githubusercontent.com/u/24811131?v=4" width="100;" alt="daixiangzi"/>
                    <br />
                    <sub><b>Daixiangzi</b></sub>
                </a>
            </td>
            <td align="center">
                <a href="https://github.com/hongyan-star">
                    <img src="https://avatars.githubusercontent.com/u/30431964?v=4" width="100;" alt="hongyan-star"/>
                    <br />
                    <sub><b>hongyan</b></sub>
                </a>
            </td>
            <td align="center">
                <a href="https://github.com/anxiangsir">
                    <img src="https://avatars.githubusercontent.com/u/31175974?v=4" width="100;" alt="anxiangsir"/>
                    <br />
                    <sub><b>Xiang An</b></sub>
                </a>
            </td>
            <td align="center">
                <a href="https://github.com/yiyexy">
                    <img src="https://avatars.githubusercontent.com/u/35927125?v=4" width="100;" alt="yiyexy"/>
                    <br />
                    <sub><b>Yiyexy</b></sub>
                </a>
            </td>
            <td align="center">
                <a href="https://github.com/SNHIPOW">
                    <img src="https://avatars.githubusercontent.com/u/62653813?v=4" width="100;" alt="SNHIPOW"/>
                    <br />
                    <sub><b>Athinklo</b></sub>
                </a>
            </td>
		</tr>
		<tr>
            <td align="center">
                <a href="https://github.com/tanhuajie">
                    <img src="https://avatars.githubusercontent.com/u/68807603?v=4" width="100;" alt="tanhuajie"/>
                    <br />
                    <sub><b>Tanhuajie</b></sub>
                </a>
            </td>
            <td align="center">
                <a href="https://github.com/ZhaoYan-ai">
                    <img src="https://avatars.githubusercontent.com/u/91243333?v=4" width="100;" alt="ZhaoYan-ai"/>
                    <br />
                    <sub><b>ZhaoYan-ai</b></sub>
                </a>
            </td>
		</tr>
	<tbody>
</table>
<!-- readme: collaborators,contributors -end -->
</section>
<section id="dataset-contributors">
<h2>Dataset Contributors<a class="headerlink" href="#dataset-contributors" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>This project would not have been possible without the invaluable contributions of the following individuals, who have been instrumental in data scraping and collection:<br />
Thank you to all the contributors for their hard work and dedication!</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Contributor</p></th>
<th class="head"><p>Emial</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Bin Qin</strong></p></td>
<td><p>skyqin&#64;gmail.com</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Lan Wu</strong></p></td>
<td><p>bah-wl&#64;hotmail.com</p></td>
</tr>
<tr class="row-even"><td><p><strong>Haiqiang Jiang</strong></p></td>
<td><p>haiqiangjiang&#64;deepglint.com</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Yuling Wu</strong></p></td>
<td><p>yulingwu&#64;deepglint.com</p></td>
</tr>
</tbody>
</table>
</section>
<section id="citation">
<h2>Citation<a class="headerlink" href="#citation" title="Permalink to this heading">ÔÉÅ</a></h2>
<div class="highlight-latex notranslate"><div class="highlight"><pre><span></span>@inproceedings<span class="nb">{</span>anxiang<span class="nb">_</span>2024<span class="nb">_</span>mlcd,
  title=<span class="nb">{</span>Multi-label Cluster Discrimination for Visual Representation Learning<span class="nb">}</span>,
  author=<span class="nb">{</span>An, Xiang and Yang, Kaicheng and Dai, Xiangzi and Feng, Ziyong and Deng, Jiankang<span class="nb">}</span>,
  booktitle=<span class="nb">{</span>ECCV<span class="nb">}</span>,
  year=<span class="nb">{</span>2024<span class="nb">}</span>
<span class="nb">}</span>
@inproceedings<span class="nb">{</span>anxiang<span class="nb">_</span>2023<span class="nb">_</span>unicom,
  title=<span class="nb">{</span>Unicom: Universal and Compact Representation Learning for Image Retrieval<span class="nb">}</span>,
  author=<span class="nb">{</span>An, Xiang and Deng, Jiankang and Yang, Kaicheng and Li, Jiawei and Feng, Ziyong and Guo, Jia and Yang, Jing and Liu, Tongliang<span class="nb">}</span>,
  booktitle=<span class="nb">{</span>ICLR<span class="nb">}</span>,
  year=<span class="nb">{</span>2023<span class="nb">}</span>
<span class="nb">}</span>
@inproceedings<span class="nb">{</span>anxiang<span class="nb">_</span>2022<span class="nb">_</span>partialfc,
    author=<span class="nb">{</span>An, Xiang and Deng, Jiankang and Guo, Jia and Feng, Ziyong and Zhu, XuHan and Yang, Jing and Liu, Tongliang<span class="nb">}</span>,
    title=<span class="nb">{</span>Killing Two Birds With One Stone: Efficient and Robust Training of Face Recognition CNNs by Partial FC<span class="nb">}</span>,
    booktitle=<span class="nb">{</span>CVPR<span class="nb">}</span>,
    year=<span class="nb">{</span>2022<span class="nb">}</span>,
<span class="nb">}</span>
@inproceedings<span class="nb">{</span>deng<span class="nb">_</span>2019<span class="nb">_</span>arcface,
  title=<span class="nb">{</span>Arcface: Additive angular margin loss for deep face recognition<span class="nb">}</span>,
  author=<span class="nb">{</span>Deng, Jiankang and Guo, Jia and Xue, Niannan and Zafeiriou, Stefanos<span class="nb">}</span>,
  booktitle=<span class="nb">{</span>CVPR<span class="nb">}</span>,
  year=<span class="nb">{</span>2019<span class="nb">}</span>
<span class="nb">}</span>
</pre></div>
</div>
</section>
<section id="acknowledgement">
<h2>Acknowledgement<a class="headerlink" href="#acknowledgement" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>We extend our deepest gratitude to the creators and contributors of the following projects:</p>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://github.com/LLaVA-VL/LLaVA-NeXT">llava-next</a>: The comprehensive codebase for training Vision-Language Models (VLMs).</p></li>
<li><p><a class="reference external" href="https://github.com/EvolvingLMMs-Lab/lmms-eval">lmms-eval</a>: The robust tool for evaluating Vision-Language Models (VLMs).</p></li>
<li><p><a class="reference external" href="https://github.com/facebookresearch/open-eqa">OpenEQA</a>: A wonderful benchmark for Embodied Question Answering.</p></li>
<li><p><a class="reference external" href="https://github.com/google-deepmind/robovqa">RoboVQA</a>: Provide high level reasoning model and dataset for robotics.</p></li>
</ol>
<p>Their exceptional work has been instrumental to our research and development efforts.</p>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, zhaoyan.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>